{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, StringType,DoubleType\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "import fileinput\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = spark.read.csv(\"Token1_Falcon_HC_Model_Data_CSV.csv\",inferSchema=False,header=True)\n",
    "\n",
    "#Indexing the received data\n",
    "indexer = StringIndexer(inputCols=[\n",
    "    'Solution Key',\n",
    "    'Error Information',\n",
    "    'Source Line Number',\n",
    "    'Remote IP Address',\n",
    "    'Program/Method/Function Module',\n",
    "    'Package',\n",
    "    'Name of Method or Function Module',\n",
    "    'Name of Class or Program',\n",
    "    'Message Area',\n",
    "    'Message number',\n",
    "    'Expiry Date',\n",
    "    'Error Subcategory',\n",
    "    'Error Short Text',\n",
    "    'Application component ID',\n",
    "    'Application Area',\n",
    "    'ABAP Name of Consumer or Server Proxy',\n",
    "    'Error Log Information',\n",
    "    'Sender party',\n",
    "    'Sender interface operation',\n",
    "    'Sender interface namespace',\n",
    "    'Sender interface name',\n",
    "    'Receiver interface operation',\n",
    "    'Receiver interface namespace',\n",
    "    'Receiver interface name'\n",
    "],\n",
    "                        outputCols=[\n",
    "    'label',\n",
    "    'Error Information_index',\n",
    "    'Source Line Number_index',\n",
    "    'Remote IP Address_index',\n",
    "    'Program/Method/Function Module_index',\n",
    "    'Package_index',\n",
    "    'Name of Method or Function Module_index',\n",
    "    'Name of Class or Program_index',\n",
    "    'Message Area_index',\n",
    "    'Message number_index',\n",
    "    'Expiry Date_index',\n",
    "    'Error Subcategory_index',\n",
    "    'Error Short Text_index',\n",
    "    'Application component ID_index',\n",
    "    'Application Area_index',\n",
    "    'ABAP Name of Consumer or Server Proxy_index',\n",
    "    'Error Log Information_index',\n",
    "    'Sender party_index',\n",
    "    'Sender interface operation_index',\n",
    "    'Sender interface namespace_index',\n",
    "    'Sender interface name_index',\n",
    "    'Receiver interface operation_index',\n",
    "    'Receiver interface namespace_index',\n",
    "    'Receiver interface name_index']).setHandleInvalid(\"keep\")\n",
    "# Getting the training data\n",
    "indexed_train = indexer.fit(df_train).transform(df_train)\n",
    "# getting The Testing Data\n",
    "indexed_test = indexed_train.where(indexed_train.Index>113)\n",
    "\n",
    "#Extracting Features and vectorize them\n",
    "numericCols = [\n",
    "    'Error Information_index',\n",
    "    'Source Line Number_index',\n",
    "    'Remote IP Address_index',\n",
    "    'Program/Method/Function Module_index',\n",
    "    'Package_index',\n",
    "    'Name of Method or Function Module_index',\n",
    "    'Name of Class or Program_index',\n",
    "    'Message Area_index',\n",
    "    'Message number_index',\n",
    "    'Expiry Date_index',\n",
    "    'Error Subcategory_index',\n",
    "    'Error Short Text_index',\n",
    "    'Application component ID_index',\n",
    "    'Application Area_index',\n",
    "    'ABAP Name of Consumer or Server Proxy_index',\n",
    "    'Error Log Information_index',\n",
    "    'Sender party_index',\n",
    "    'Sender interface operation_index',\n",
    "    'Sender interface namespace_index',\n",
    "    'Sender interface name_index',\n",
    "    'Receiver interface operation_index',\n",
    "    'Receiver interface namespace_index',\n",
    "    'Receiver interface name_index'\n",
    "]\n",
    "assembler_train = VectorAssembler(inputCols=numericCols, outputCol=\"features\")\n",
    "indexed_train = assembler_train.transform(indexed_train)\n",
    "assembler_test = VectorAssembler(inputCols=numericCols, outputCol=\"features\")\n",
    "indexed_test = assembler_test.transform(indexed_test)\n",
    "\n",
    "# Training the Naive Bayes Model\n",
    "nb = NaiveBayes(featuresCol='features', labelCol='label')\n",
    "nb_pipeline = Pipeline(stages=[nb])\n",
    "nb_model = nb_pipeline.fit(indexed_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o572.save.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.hadoop.mapred.FileOutputCommitter.setupJob(FileOutputCommitter.java:131)\r\n\tat org.apache.hadoop.mapred.OutputCommitter.setupJob(OutputCommitter.java:265)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:79)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1599)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1599)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1585)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1585)\r\n\tat org.apache.spark.ml.util.DefaultParamsWriter$.saveMetadata(ReadWrite.scala:413)\r\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$saveImpl$1(Pipeline.scala:250)\r\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$saveImpl$1$adapted(Pipeline.scala:247)\r\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\r\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.saveImpl(Pipeline.scala:247)\r\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.saveImpl(Pipeline.scala:346)\r\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:168)\r\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.super$save(Pipeline.scala:344)\r\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$4(Pipeline.scala:344)\r\n\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent(events.scala:174)\r\n\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent$(events.scala:169)\r\n\tat org.apache.spark.ml.util.Instrumentation.withSaveInstanceEvent(Instrumentation.scala:42)\r\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3(Pipeline.scala:344)\r\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3$adapted(Pipeline.scala:344)\r\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\r\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.save(Pipeline.scala:344)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:343)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:344)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:901)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 22 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# cvModel = cv_grid.fit(train_df)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[39m# save best model to specified path\u001b[39;00m\n\u001b[0;32m      4\u001b[0m mPath \u001b[39m=\u001b[39m  \u001b[39m\"\u001b[39m\u001b[39m/path/to/model/folder\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> 5\u001b[0m nb_model\u001b[39m.\u001b[39;49mwrite()\u001b[39m.\u001b[39;49moverwrite()\u001b[39m.\u001b[39;49msave(mPath)\n",
      "File \u001b[1;32mc:\\Users\\I575853\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\ml\\util.py:197\u001b[0m, in \u001b[0;36mJavaMLWriter.save\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(path, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    196\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mpath should be a string, got type \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(path))\n\u001b[1;32m--> 197\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jwrite\u001b[39m.\u001b[39;49msave(path)\n",
      "File \u001b[1;32mc:\\Users\\I575853\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32mc:\\Users\\I575853\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    189\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    191\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    192\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\I575853\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o572.save.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.hadoop.mapred.FileOutputCommitter.setupJob(FileOutputCommitter.java:131)\r\n\tat org.apache.hadoop.mapred.OutputCommitter.setupJob(OutputCommitter.java:265)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:79)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1599)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1599)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1585)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1585)\r\n\tat org.apache.spark.ml.util.DefaultParamsWriter$.saveMetadata(ReadWrite.scala:413)\r\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$saveImpl$1(Pipeline.scala:250)\r\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$saveImpl$1$adapted(Pipeline.scala:247)\r\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\r\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.saveImpl(Pipeline.scala:247)\r\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.saveImpl(Pipeline.scala:346)\r\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:168)\r\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.super$save(Pipeline.scala:344)\r\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$4(Pipeline.scala:344)\r\n\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent(events.scala:174)\r\n\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent$(events.scala:169)\r\n\tat org.apache.spark.ml.util.Instrumentation.withSaveInstanceEvent(Instrumentation.scala:42)\r\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3(Pipeline.scala:344)\r\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3$adapted(Pipeline.scala:344)\r\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\r\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.save(Pipeline.scala:344)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:343)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:344)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:901)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 22 more\r\n"
     ]
    }
   ],
   "source": [
    "# cvModel = cv_grid.fit(train_df)\n",
    "\n",
    "# save best model to specified path\n",
    "mPath =  \"/path/to/model/folder\"\n",
    "nb_model.bestModel.write().overwrite().save(mPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokennizeData(data):\n",
    "    sw = stopwords.words('english')\n",
    "    # remove stop words from the string\n",
    "    X_list = word_tokenize(data)\n",
    "    X_set = {w for w in X_list if not w in sw}\n",
    "    return X_set\n",
    "\n",
    "# To get data from master Payload\n",
    "def fileAnalyzer(filename,masterDict):\n",
    "    errorInformationVar=\"\"\n",
    "    for line in fileinput.input(files=filename, encoding=\"utf-8\"):\n",
    "        if(line.startswith(\"Receiver interface name:\", 0)):\n",
    "            data = line[25:-1]\n",
    "\n",
    "            data = data.strip()\n",
    "            data = data.strip('\" \\\"')\n",
    "\n",
    "            masterDict['Receiver interface name'] = (\n",
    "                data if data != \"\" else 'nan')\n",
    "            # if(isErrorData):\n",
    "            #     global receiver\n",
    "            #     receiver = data if data!=\"\" else 'nan'\n",
    "        elif(line.startswith(\"Receiver interface namespace:\", 0)):\n",
    "            data = line[30:-1]\n",
    "            data = data.strip()\n",
    "            data = data.strip('\" \\\"')\n",
    "\n",
    "            masterDict['Receiver interface namespace'] = (\n",
    "                data if data != \"\" else 'nan')\n",
    "        elif(line.startswith(\"Receiver interface operation:\", 0)):\n",
    "            data = line[30:-1]\n",
    "            data = data.strip()\n",
    "            data = data.strip('\" \\\"')\n",
    "\n",
    "            masterDict['Receiver interface operation'] = (\n",
    "                data if data != \"\" else 'nan')\n",
    "        elif(line.startswith(\"Sender interface name:\", 0)):\n",
    "            data = line[23:-1]\n",
    "            data = data.strip()\n",
    "            data = data.strip('\" \\\"')\n",
    "\n",
    "            masterDict['Sender interface name'] = (\n",
    "                data if data != \"\" else 'nan')\n",
    "            # if(isErrorData):\n",
    "            #     global sender\n",
    "            #     sender=data if data!=\"\" else 'nan'\n",
    "        elif(line.startswith(\"Sender interface namespace:\", 0)):\n",
    "            data = line[28:-1]\n",
    "            data = data.strip()\n",
    "            data = data.strip('\" \\\"')\n",
    "\n",
    "            masterDict['Sender interface namespace'] = (\n",
    "                data if data != \"\" else 'nan')\n",
    "        elif(line.startswith(\"Sender interface operation:\", 0)):\n",
    "            data = line[28:-1]\n",
    "            data = data.strip()\n",
    "            data = data.strip('\" \\\"')\n",
    "\n",
    "            masterDict['Sender interface operation'] = (\n",
    "                data if data != \"\" else 'nan')\n",
    "        elif(line.startswith(\"Sender party:\", 0)):\n",
    "            data = line[14:-1]\n",
    "            data = data.strip()\n",
    "            data = data.strip('\" \\\"')\n",
    "\n",
    "            masterDict['Sender party'] = (data if data != \"\" else 'nan')\n",
    "        elif(line.startswith(\"Error Log Information:\", 0)):\n",
    "            data = line[23:-1]\n",
    "            data = data.strip()\n",
    "            data = data.strip('\" \\\"')\n",
    "\n",
    "            data = tokennizeData(data)\n",
    "            data=sorted(data,key=str.casefold)\n",
    "            masterDict['Error Log Information'] = (\n",
    "                data if data != \"\" else 'nan')\n",
    "        elif(line.startswith(\"Error information:\", 0)):\n",
    "            \n",
    "            data = line[19:-1]\n",
    "            errorInformationVar=data\n",
    "            data = data.strip()\n",
    "            data = data.strip('\" \\\"')\n",
    "\n",
    "            data = tokennizeData(data)\n",
    "            data=sorted(data,key=str.casefold)\n",
    "            masterDict['Error Information'] = (\n",
    "                data if data != \"\" else 'nan')\n",
    "        elif(line.startswith(\"ABAP Name of Consumer or Server Proxy:\", 0)):\n",
    "            data = line[40:-1]\n",
    "            data = data.strip()\n",
    "            data = data.strip('\" \\\"')\n",
    "\n",
    "\n",
    "            masterDict['ABAP Name of Consumer or Server Proxy'] = (\n",
    "                data if data != \"\" else 'nan')\n",
    "        elif(line.startswith(\"Application Area:\", 0)):\n",
    "            data = line[18:-1]\n",
    "            data = data.strip()\n",
    "            data = data.strip('\" \\\"')\n",
    "\n",
    "\n",
    "            masterDict['Application Area'] = (data if data != \"\" else 'nan')\n",
    "        elif(line.startswith(\"Application component ID:\", 0)):\n",
    "            data = line[26:-1]\n",
    "            data = data.strip()\n",
    "            data = data.strip('\" \\\"')\n",
    "\n",
    "            masterDict['Application component ID'] = (\n",
    "                data if data != \"\" else 'nan')\n",
    "        elif(line.startswith(\"Error Short Text:\", 0)):\n",
    "            data = line[18:-1]\n",
    "            data = data.strip()\n",
    "            data = data.strip('\" \\\"')\n",
    "\n",
    "            data = tokennizeData(data)\n",
    "            data=sorted(data,key=str.casefold)\n",
    "            masterDict['Error Short Text'] = (data if data != \"\" else 'nan')\n",
    "            # masterDict['PAF Error']=(data if data != \"\" else 'nan' )\n",
    "        elif(line.startswith(\"Error Subcategory:\", 0)):\n",
    "            data = line[19:-1]\n",
    "            data = data.strip()\n",
    "            data = data.strip('\" \\\"')\n",
    "\n",
    "            data = tokennizeData(data)\n",
    "            data=sorted(data,key=str.casefold)\n",
    "            masterDict['Error Subcategory'] = (\n",
    "                data if data != \"\" else 'nan')\n",
    "        elif(line.startswith(\"Expiry Date:\", 0)):\n",
    "            data = line[13:-1]\n",
    "            data = data.strip()\n",
    "            data = data.strip('\" \\\"')\n",
    "\n",
    "            masterDict['Expiry Date'] = (data if data != \"\" else 'nan')\n",
    "        elif(line.startswith(\"Message number:\", 0)):\n",
    "            data = str(line[16:-1])\n",
    "            data = data.strip()\n",
    "            data = data.strip('\" \\\"')\n",
    "\n",
    "            masterDict['Message number'] = (data if data != \"\" else 'nan')\n",
    "        elif(line.startswith(\"Message Area:\", 0)):\n",
    "            data = line[14:-1]\n",
    "            data = data.strip()\n",
    "            data = data.strip('\" \\\"')\n",
    "\n",
    "            masterDict['Message Area'] = (data if data != \"\" else 'nan')\n",
    "        elif(line.startswith(\"Name of Class or Program:\", 0)):\n",
    "            data = line[26:-1]\n",
    "            data = data.strip()\n",
    "            data = data.strip('\" \\\"')\n",
    "\n",
    "            masterDict['Name of Class or Program'] = (\n",
    "                data if data != \"\" else 'nan')\n",
    "        elif(line.startswith(\"Name of Method or Function Module:\", 0)):\n",
    "            data = line[35:-1]\n",
    "            data = data.strip()\n",
    "            data = data.strip('\" \\\"')\n",
    "\n",
    "            masterDict['Name of Method or Function Module'] = (\n",
    "                data if data != \"\" else 'nan')\n",
    "        elif(line.startswith(\"Package:\", 0)):\n",
    "            data = line[9:-1]\n",
    "            data = data.strip()\n",
    "            data = data.strip('\" \\\"')\n",
    "\n",
    "            masterDict['Package'] = (data if data != \"\" else 'nan')\n",
    "        elif(line.startswith(\"Program/Method/Function Module:\", 0)):\n",
    "            data = line[32:-1]\n",
    "            data = data.strip()\n",
    "            data = data.strip('\" \\\"')\n",
    "            # print(data)\n",
    "            masterDict['Program/Method/Function Module'] = (\n",
    "                data if data != \"\" else 'nan')\n",
    "        elif(line.startswith(\"Remote IP Address:\", 0)):\n",
    "            data = line[19:-1]\n",
    "            data = data.strip()\n",
    "            data = data.strip('\" \\\"')\n",
    "\n",
    "            masterDict['Remote IP Address'] = (\n",
    "                data if data != \"\" else 'nan')\n",
    "        elif(line.startswith(\"Source Line Number:\", 0)):\n",
    "            # print(line)\n",
    "            data = (line[20:])\n",
    "            # print(data)\n",
    "            data = data.strip()\n",
    "            data = data.strip('\" \\\"')\n",
    "\n",
    "            masterDict['Source Line Number'] = (\n",
    "                data if data != \"\" else 'nan')\n",
    "    fileinput.close()\n",
    "    return errorInformationVar\n",
    "\n",
    "\n",
    "# Processing The master Payload\n",
    "def inputPayload():\n",
    "    result={\n",
    "        'sender':\"\",\n",
    "        'recv':\"\",\n",
    "        'Error':\"\",\n",
    "        'ErrorId':\"\",\n",
    "        'UniqueKeys':[],\n",
    "        'PercentageMatch':[],\n",
    "        'ResolutionsSteps':[]\n",
    "    }\n",
    "    masterDict={\n",
    "    'Solution Key':'nan',\n",
    "    'Error Information':'nan',\n",
    "    'Source Line Number':'nan',\n",
    "    'Remote IP Address':'nan',\n",
    "    'Program/Method/Function Module':'nan',\n",
    "    'Package':'nan',\n",
    "    'Name of Method or Function Module':'nan',\n",
    "    'Name of Class or Program':'nan',\n",
    "    'Message Area':'nan',\n",
    "    'Message number':'nan',\n",
    "    'Expiry Date':'nan',\n",
    "    'Error Subcategory':'nan',\n",
    "    'Error Short Text':'nan',\n",
    "    'Application component ID':'nan',\n",
    "    'Application Area':'nan',\n",
    "    'ABAP Name of Consumer or Server Proxy':'nan',\n",
    "    'Error Log Information':'nan',\n",
    "    'Sender party':'nan',\n",
    "    'Sender interface operation':'nan',\n",
    "    'Sender interface namespace':'nan',\n",
    "    'Sender interface name':'nan',\n",
    "    'Receiver interface operation':'nan',\n",
    "    'Receiver interface namespace':'nan',\n",
    "    'Receiver interface name':'nan'\n",
    "    }\n",
    "\n",
    "    # with open('errorInput.txt', 'w' ,encoding=\"utf-8\") as f:\n",
    "    #        f.write(masterPayload)\n",
    "    # f.close()\n",
    "    result['Error']= fileAnalyzer('errorInput.txt',masterDict)\n",
    "    result['sender']=masterDict['Sender interface name']\n",
    "    result['recv']=masterDict['Receiver interface name']\n",
    "    # result['Error']=masterDict['Error Information']\n",
    "    result['ErrorId']=masterDict['Message Area']+\":\"+masterDict['Message number']\n",
    "    df = pd.DataFrame.from_dict(masterDict, orient='index')\n",
    "    df = df.transpose()\n",
    "    # Converting Data to CSV\n",
    "    df.to_csv('my_file.csv', index=False, header=True)\n",
    "\n",
    "    result['UniqueKeys'].append(getPredictionForTest())\n",
    "    print(result)\n",
    "\n",
    "def top3_index(lst, rev=True):\n",
    "    index = range(len(lst))\n",
    "    s = sorted(index, reverse=rev, key=lambda i: lst[i])\n",
    "    return s[:3]\n",
    "\n",
    "\n",
    "def getPredictionForTest():\n",
    "    df_to_pred = spark.read.csv(\"my_file.csv\",inferSchema=False,header=True)\n",
    "    indexed_to_pred = indexer.fit(df_train).transform(df_to_pred)\n",
    "    assembler_to_pred = VectorAssembler(inputCols=numericCols, outputCol=\"features\")\n",
    "    indexed_to_pred = assembler_to_pred.transform(indexed_to_pred)\n",
    "    nb_predictions=nb_model.transform(indexed_to_pred)\n",
    "    nb_prob_list=nb_predictions.collect()[0][50]\n",
    "    top3_Lebel_res=top3_index(nb_prob_list)\n",
    "    top3_res=[]\n",
    "    for x in top3_Lebel_res:\n",
    "        percentage=(round(nb_prob_list[x],2)*100)\n",
    "        uniq_key=(indexed_train.filter(indexed_train.label==x).collect()[0][1])\n",
    "        top3_res.append((percentage,uniq_key))\n",
    "    print(top3_res)\n",
    "    return top3_res\n",
    "\n",
    "    # pred=nb_predictions.collect()[0][51]\n",
    "    # result=indexed_train.filter(indexed_train.label==pred).collect()[0][1]\n",
    "    # return (result,80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(100.0, 'SCM_FALCON029_2022'), (0.0, 'SCM_FALCON040_2023'), (0.0, 'SCM_FALCON044_2023')]\n",
      "{'sender': 'LogisticsExecutionControlFulfilmentOut', 'recv': 'InboundDeliveryProcessingFulfilmentIn', 'Error': 'User is locked. Please notify the person responsible', 'ErrorId': 'APDL_MO_TASK:004', 'UniqueKeys': [[(100.0, 'SCM_FALCON029_2022'), (0.0, 'SCM_FALCON040_2023'), (0.0, 'SCM_FALCON044_2023')]], 'PercentageMatch': [], 'ResolutionsSteps': []}\n"
     ]
    }
   ],
   "source": [
    "inputPayload()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
